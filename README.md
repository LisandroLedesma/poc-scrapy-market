# Supermarket Scraper - Test

## DescripciÃ³n
Este proyecto es un **web scraper** desarrollado con **Scrapy** y **Selenium**, diseÃ±ado para extraer productos y sus precios de diferentes sucursales de un supermercado online. El objetivo principal del proyecto es automatizar la recolecciÃ³n de datos de productos de diversas sucursales mediante tÃ©cnicas de scraping, con el soporte de inicio de sesiÃ³n y selecciÃ³n de sucursales a travÃ©s de Selenium.
Fue parte de una POC que hice para mi proyecto final de la carrera IngenierÃ­a en Sistemas de InformaciÃ³n, se siguiÃ³ iterando pero lo dejÃ³ acÃ¡ porque se me hizo bonito ğŸ˜.


## TecnologÃ­as Utilizadas ğŸ‘¨â€ğŸ’»
- **Python**: Lenguaje principal.
- **Scrapy**: Framework para scraping web.
- **Selenium**: Para manejar el inicio de sesiÃ³n y la navegaciÃ³n de sitios web dinÃ¡micos.
- **Poetry**: Manejo de dependencias y gestiÃ³n del entorno virtual.
- **.env**: Manejo de variables de entorno para credenciales y configuraciones.

## InstalaciÃ³n ğŸ‘ˆ

1. **Clona este repositorio** ğŸ‘ˆ

    ```bash
    git clone https://github.com/tu-usuario/supermarket-scraper.git
    cd supermarket-scraper
    ```

2. **Instala las dependencias utilizando Poetry** ğŸ‘ˆ

    ```bash
    poetry install
    ```

3. **Crea un archivo `.env`** en la raÃ­z del proyecto con las siguientes variables de entorno ğŸ‘ˆ

    ```bash
    LOGIN_EMAIL=tu_email
    LOGIN_PASSWORD=tu_contraseÃ±a
    BRANCHES=["Ruta 20", "Otra Sucursal", "Tercera Sucursal"]
    ```

## Uso

1. **Ejecuta el scraper** con el siguiente comando ğŸ¤–

    ```bash
    poetry run scrapy crawl supermami -o productos.json
    ```

    Esto iniciarÃ¡ el proceso de scraping para las sucursales listadas en el archivo `.env` y generarÃ¡ un archivo `productos.json` con los datos extraÃ­dos.
    AclaraciÃ³n importante, tengo las urls que querÃ­a revisar para la POC.

